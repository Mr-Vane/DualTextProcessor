# DualTextProcessor

## ğŸ“ DescripciÃ³n del Proyecto

**DualTextProcessor** es una herramienta de procesamiento de lenguaje natural (PLN) escrita en Python, diseÃ±ada para preparar conjuntos de datos de texto bilingÃ¼es o de doble columna para su uso en modelos de *Machine Learning* o *Deep Learning*, como los modelos de traducciÃ³n automÃ¡tica o *Sequence-to-Sequence*.

El proyecto automatiza dos pasos cruciales en el preprocesamiento de datos:

1. **TokenizaciÃ³n**: Convierte las palabras de las columnas de texto en secuencias de tokens numÃ©ricos, creando un vocabulario y un mapeo de tokens.

1. **Padding**: Aplica relleno (padding) a las secuencias tokenizadas para asegurar que todas las entradas y salidas tengan una longitud uniforme, lo cual es un requisito comÃºn para el entrenamiento de redes neuronales recurrentes (RNN) o transformadores.

## ğŸš€ CaracterÃ­sticas Principales

- **Procesamiento de Doble Columna**: DiseÃ±ado especÃ­ficamente para manejar conjuntos de datos con dos columnas de texto (por ejemplo, idioma de origen e idioma de destino).

- **GeneraciÃ³n de Vocabulario**: Crea un vocabulario Ãºnico y guarda los mapeos de palabras a tokens (`tokens.json`) y de tokens a palabras (`token_to_word.json`).

- **Tokens Especiales**: Incluye tokens especiales esenciales para modelos de secuencia:
  - `<pad>`: Relleno (Token 0)
  - `<bos>`: Inicio de secuencia (Token 1)
  - `<eos>`: Fin de secuencia (Token 2)
  - `<unk>`: Palabra desconocida (Token 3)

- **Longitud Ã“ptima**: Calcula las longitudes de secuencia Ã³ptimas para el padding basÃ¡ndose en la distribuciÃ³n de los datos.

- **Salida Estructurada**: Genera archivos CSV con los datos tokenizados y con padding, listos para ser cargados en un *framework* de *Deep Learning*.



## ğŸ› ï¸ InstalaciÃ³n

### Requisitos

AsegÃºrate de tener Python instalado en tu sistema.

El Ãºnico requisito de librerÃ­a externa es `numpy`:

```bash
pip install numpy>=1.21.0
```

### Estructura del Proyecto

El proyecto tiene la siguiente estructura de directorios:

```
DualTextProcessor/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ database/             # Contiene los datasets procesados
â”‚   â”œâ”€â”€ tokens/               # Contiene los archivos de mapeo de tokens
â”œâ”€â”€ src/                      # MÃ³dulos principales del procesador
â”‚   â”œâ”€â”€ name.py               # LÃ³gica para la lectura de CSV
â”‚   â”œâ”€â”€ padder.py             # LÃ³gica para el cÃ¡lculo y aplicaciÃ³n de padding
â”‚   â””â”€â”€ tokenizador.py        # LÃ³gica para la tokenizaciÃ³n y creaciÃ³n de vocabulario
â”œâ”€â”€ main.py                   # Punto de entrada principal
â””â”€â”€ requirements.txt          # Dependencias del proyecto
```



## ğŸ’» Uso

### 1. PreparaciÃ³n del Dataset

Coloca tu archivo CSV de entrada (con las dos columnas de texto a procesar) en el directorio raÃ­z del proyecto o asegÃºrate de proporcionar la ruta correcta.

### 2. EjecuciÃ³n

Ejecuta el script principal `main.py` proporcionando el nombre de tu archivo de base de datos como argumento.

```bash
python main.py <nombre_de_tu_dataset.csv>
```

**Ejemplo:**

Si tu archivo se llama `mi_dataset.csv`, el comando serÃ­a:

```bash
python main.py mi_dataset.csv
```

### 3. Salida

Tras la ejecuciÃ³n, se generarÃ¡n los siguientes archivos en el directorio `data/database/`:

- `dataset_tokenizado.csv`: El dataset con las columnas de texto reemplazadas por secuencias de tokens numÃ©ricos.

- `dataset_tokenizado_padded.csv`: El dataset final con las secuencias tokenizadas y con padding aplicado, listo para el entrenamiento.

AdemÃ¡s, se crearÃ¡n los archivos de mapeo de vocabulario en `data/tokens/`:

- `tokens.json`: Mapeo de palabra a token (`word_to_token`).

- `token_to_word.json`: Mapeo de token a palabra (`token_to_word`).



## âš™ï¸ MÃ³dulos Internos

| Archivo | DescripciÃ³n |
| --- | --- |
| `main.py` | Orquesta el flujo de trabajo: lee el dataset, llama al tokenizador, calcula las longitudes Ã³ptimas y aplica el padding. |
| `src/tokenizador.py` | Contiene la lÃ³gica para iterar sobre el dataset, construir el vocabulario y convertir las palabras en tokens numÃ©ricos. |
| `src/padder.py` | Implementa las funciones para determinar la longitud mÃ¡xima de las secuencias y aplicar el relleno (`<pad>`) a las secuencias tokenizadas. |
| `src/name.py` | Funciones auxiliares, principalmente para la lectura y manejo de las columnas del archivo CSV de entrada. |

## ğŸ¤ Contribuciones

Cualquier contribucion es bienvenida.
Las contribuciones son bienvenidas.