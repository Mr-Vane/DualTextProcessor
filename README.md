# DualTextProcessor

## üìù Descripci√≥n del Proyecto

**DualTextProcessor** es una herramienta de procesamiento de lenguaje natural (PLN) escrita en Python, dise√±ada para preparar conjuntos de datos de texto biling√ºes o de doble columna para su uso en modelos de *Machine Learning* o *Deep Learning*, como los modelos de traducci√≥n autom√°tica o *Sequence-to-Sequence*.

El proyecto automatiza dos pasos cruciales en el preprocesamiento de datos:

1. **Tokenizaci√≥n**: Convierte las palabras de las columnas de texto en secuencias de tokens num√©ricos, creando un vocabulario y un mapeo de tokens.

1. **Padding**: Aplica relleno (padding) a las secuencias tokenizadas para asegurar que todas las entradas y salidas tengan una longitud uniforme, lo cual es un requisito com√∫n para el entrenamiento de redes neuronales recurrentes (RNN) o transformadores.

## üöÄ Caracter√≠sticas Principales

- **Procesamiento de Doble Columna**: Dise√±ado espec√≠ficamente para manejar conjuntos de datos con dos columnas de texto (por ejemplo, idioma de origen e idioma de destino).

- **Generaci√≥n de Vocabulario**: Crea un vocabulario √∫nico y guarda los mapeos de palabras a tokens (`tokens.json`) y de tokens a palabras (`token_to_word.json`).

- **Tokens Especiales**: Incluye tokens especiales esenciales para modelos de secuencia:
  - `<pad>`: Relleno (Token 0)
  - `<bos>`: Inicio de secuencia (Token 1)
  - `<eos>`: Fin de secuencia (Token 2)
  - `<unk>`: Palabra desconocida (Token 3)

- **Longitud √ìptima**: Calcula las longitudes de secuencia √≥ptimas para el padding bas√°ndose en la distribuci√≥n de los datos.

- **Salida Estructurada**: Genera archivos CSV con los datos tokenizados y con padding, listos para ser cargados en un *framework* de *Deep Learning*.



## üõ†Ô∏è Instalaci√≥n

### Requisitos

Aseg√∫rate de tener Python instalado en tu sistema.

```bash
pip install -r requirements.txt
```

### Estructura del Proyecto

El proyecto tiene la siguiente estructura de directorios:

```
DualTextProcessor/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ database/             # Contiene los datasets procesados
‚îÇ   ‚îú‚îÄ‚îÄ tokens/               # Contiene los archivos de mapeo de tokens
‚îú‚îÄ‚îÄ src/                      # M√≥dulos principales del procesador
‚îÇ   ‚îú‚îÄ‚îÄ name.py               # L√≥gica para la lectura de CSV
‚îÇ   ‚îú‚îÄ‚îÄ padder.py             # L√≥gica para el c√°lculo y aplicaci√≥n de padding
‚îÇ   ‚îî‚îÄ‚îÄ tokenizador.py        # L√≥gica para la tokenizaci√≥n y creaci√≥n de vocabulario
‚îú‚îÄ‚îÄ main.py                   # Punto de entrada principal
‚îî‚îÄ‚îÄ requirements.txt          # Dependencias del proyecto
```



## üíª Uso

### 1. Preparaci√≥n del Dataset

Coloca tu archivo CSV de entrada (con las dos columnas de texto a procesar) en el directorio ra√≠z del proyecto o aseg√∫rate de proporcionar la ruta correcta.

### 2. Ejecuci√≥n

Ejecuta el script principal `main.py` proporcionando el nombre de tu archivo de base de datos como argumento.

```bash
python main.py <nombre_de_tu_dataset.csv>
```

**Ejemplo:**

Si tu archivo se llama `mi_dataset.csv`, el comando ser√≠a:

```bash
python main.py mi_dataset.csv
```

### 3. Salida

Tras la ejecuci√≥n, se generar√°n los siguientes archivos en el directorio `data/database/`:

- `dataset_tokenizado.csv`: El dataset con las columnas de texto reemplazadas por secuencias de tokens num√©ricos.

- `dataset_tokenizado_padded.csv`: El dataset final con las secuencias tokenizadas y con padding aplicado, listo para el entrenamiento.

Adem√°s, se crear√°n los archivos de mapeo de vocabulario en `data/tokens/`:

- `tokens.json`: Mapeo de palabra a token (`word_to_token`).

- `token_to_word.json`: Mapeo de token a palabra (`token_to_word`).



## ‚öôÔ∏è M√≥dulos Internos

| Archivo | Descripci√≥n |
| --- | --- |
| `main.py` | Orquesta el flujo de trabajo: lee el dataset, llama al tokenizador, calcula las longitudes √≥ptimas y aplica el padding. |
| `src/tokenizador.py` | Contiene la l√≥gica para iterar sobre el dataset, construir el vocabulario y convertir las palabras en tokens num√©ricos. |
| `src/padder.py` | Implementa las funciones para determinar la longitud m√°xima de las secuencias y aplicar el relleno (`<pad>`) a las secuencias tokenizadas. |
| `src/name.py` | Funciones auxiliares, principalmente para la lectura y manejo de las columnas del archivo CSV de entrada. |

## ü§ù Contribuciones

Cualquier contribucion es bienvenida.
